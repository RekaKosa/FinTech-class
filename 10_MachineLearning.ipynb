{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<b> \n",
    "    <font size=\"7\">\n",
    "        Computational Finance and FinTech <br><br>\n",
    "        M.Sc. International Finance\n",
    "    </font>\n",
    "</b>\n",
    "<br><br>\n",
    "<img src=\"pics/HWR.png\" width=400px>\n",
    "<br><br>\n",
    "<b>\n",
    "    <font size=\"5\"> \n",
    "        Prof. Dr. Natalie Packham <br>\n",
    "        Berlin School of Economics and Law <br>\n",
    "        Summer Term 2024\n",
    "    </font>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Machine-Learning-in-Finance\" data-toc-modified-id=\"Machine-Learning-in-Finance-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Machine Learning in Finance</a></span><ul class=\"toc-item\"><li><span><a href=\"#Unsupervised-learning\" data-toc-modified-id=\"Unsupervised-learning-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>Unsupervised learning</a></span></li><li><span><a href=\"#$k$-means-clustering\" data-toc-modified-id=\"$k$-means-clustering-10.2\"><span class=\"toc-item-num\">10.2&nbsp;&nbsp;</span>$k$-means clustering</a></span></li><li><span><a href=\"#Supervised-learning\" data-toc-modified-id=\"Supervised-learning-10.3\"><span class=\"toc-item-num\">10.3&nbsp;&nbsp;</span>Supervised learning</a></span></li><li><span><a href=\"#Gaussian-Naive-Bayes-(GNB)\" data-toc-modified-id=\"Gaussian-Naive-Bayes-(GNB)-10.4\"><span class=\"toc-item-num\">10.4&nbsp;&nbsp;</span>Gaussian Naive Bayes (GNB)</a></span></li><li><span><a href=\"#Logistic-regression-(LR)\" data-toc-modified-id=\"Logistic-regression-(LR)-10.5\"><span class=\"toc-item-num\">10.5&nbsp;&nbsp;</span>Logistic regression (LR)</a></span></li><li><span><a href=\"#Decision-trees\" data-toc-modified-id=\"Decision-trees-10.6\"><span class=\"toc-item-num\">10.6&nbsp;&nbsp;</span>Decision trees</a></span></li><li><span><a href=\"#Deep-neural-networks-(DNN's)\" data-toc-modified-id=\"Deep-neural-networks-(DNN's)-10.7\"><span class=\"toc-item-num\">10.7&nbsp;&nbsp;</span>Deep neural networks (DNN's)</a></span></li><li><span><a href=\"#Support-vector-machines-(SVM's)\" data-toc-modified-id=\"Support-vector-machines-(SVM's)-10.8\"><span class=\"toc-item-num\">10.8&nbsp;&nbsp;</span>Support vector machines (SVM's)</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning in Finance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Machine Learning\n",
    "* Further reading: __Py4Fi, Chapter 13__, from page 444.\n",
    "* The __Py4Fi__ book is very brief and application oriented, a great resource to dive deeper is the book\n",
    "> James, Witten, Hastie, Tibshirani: An Introduction to Statistical Learning. Springer, 2013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Machine Learning\n",
    "* Machine Learning is a vast field with diverse applications. \n",
    "* This chapter gives an overview and some use cases, which can be used as a starting point.\n",
    "* Machine Learning methods are split into:\n",
    "   * unsupervised learning, and\n",
    "   * supervised learning.\n",
    "* Machine Learning problems are split into: \n",
    "   * regression, and\n",
    "   * classification.\n",
    "* Python offers a number of libraries for Machine Learning:\n",
    "   * `scikit-learn` <http://scikit-learn.org>\n",
    "   * `TensorFlow` <http://tensorflow.org>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Machine Learning\n",
    "* The usual initialisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Unsupervised learning\n",
    "* The principal idea of unsupervised learning is to extract information from data without any guidance or feedback.\n",
    "* A typical appication is **clustering** (a classification problem).\n",
    "* One such algorithm is $k$-means clustering, which cluster data into $k$ subsets, called clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## $k$-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### $k$-means clustering\n",
    "* `scikit-learn` allows the creation of sample data sets for different types of ML problems. \n",
    "* Here we create a sample data set to illustrate $k$-means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=250, centers=4, random_state=500, cluster_std=1.25) # create a sample data set with 250 samples and 4 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(X[:,0], X[:,1], s=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### $k$-means clustering\n",
    "* The following code demonstrates the use of $k$-means clustering. \n",
    "* The algorithm determines $k$ clusters and assigns each sample to a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans # import model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=4, random_state=0) # instantiate the model\n",
    "model.fit(X) # fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the cluster (number) for each sample in the raw data\n",
    "y_kmeans=model.predict(X)\n",
    "y_kmeans[:12] # some cluster (numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# plot the data with one colour per cluster\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(X[:,0], X[:,1], c=y_kmeans, cmap='coolwarm');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### $k$-means clustering\n",
    "* Here is how it works (see Section 10.3 of James et al., 2013):\n",
    "* Let $x_1, x_2, \\ldots, x_n$ denote the sample of points. \n",
    "* Let $C_1, \\ldots, C_k$ denote sets containing the indices of the observations in each cluster.\n",
    "* They must satisfy the following two properties: \n",
    "* Each observation belongs to at least one cluster: $C_1\\cup C_2\\cup\\cdots \\cup C_k=\\{1, \\ldots, n\\}$. \n",
    "* Each observation belongs to no more than one cluster: $C_i\\cap C_j$ for all $i,j\\in \\{1,\\ldots, k\\}$. \n",
    "* The goal is to find $k$ cluster that minimise *within-cluster-variation*.\n",
    "* This is achieved by minimising least-square-distances (see James et al. for details)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Supervised learning\n",
    "* Most problems belong to the domain of supervised learning. \n",
    "* Here, some guidance in the form of know results or observed data is available. \n",
    "* Linear regression is one example of supervised learning.\n",
    "* Here, we will continues to study classification problems: \n",
    "   * Gaussian Naive Bayes\n",
    "   * Logistic regression\n",
    "   * Decision trees\n",
    "   * Deep neural networks\n",
    "   * Support vector machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Supervised learning\n",
    "* More formally, the setting is as follows: \n",
    "* We have *training observations* $(x_1,y_1), (x_2,y_2), \\ldots, (x_n,y_n)$.\n",
    "* (Note that $x_k$ can be vectors, i.e., $x_k \\in \\mathbb R^d$, $d\\geq 1$.)\n",
    "* Given some learning method, we estimate $\\hat f$, such that the *predictions* $\\hat f(x_1), \\hat f(x_2), \\ldots, \\hat f(x_n)$ are approximately $y_1, y_2, \\ldots, y_n$. \n",
    "* In practice, we are not so much interested in whether $\\hat f(x_i)\\approx y_i$. \n",
    "* Instead, we want to know whether $\\hat f(x_0)$ is approximately equal to $y_0$, where $(x_0,y_0)$ is a previously unseen *test* observation not used in the training stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Supervised learning - classification\n",
    "* In a classification setting, the $y_1, y_2, \\ldots, y_n$ can be qualitative data (corresponding to the classes). \n",
    "* The accuracy of the estimate $\\hat f(x_i) = \\hat y_i$ is quantified for example by the training *error rate*, which is the proportion of incorrectly classified points: \n",
    "$\\displaystyle\\frac{1}{n} \\sum_{i=1}^n \\mathbf 1_{\\{y_i\\not=\\hat y_i\\}}$\n",
    "* Alternatively, one can specify the proportion of *correctly* classified observations in the training data set: $\\displaystyle \\frac{1}{n} \\sum_{i=1}^n \\mathbf 1_{\\{y_i=\\hat y_i\\}}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The data\n",
    "* The following code produces a sample set with two features and a single binary label (0 or 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "n_samples=100\n",
    "X, y = make_classification(n_samples=n_samples, n_features=2, n_informative=2, \\\n",
    "                           n_redundant=0, n_repeated=0, random_state=250)\n",
    "X[:5] # two real-valued features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:5] # single binary label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(x=X[:,0], y=X[:,1], c=y, cmap='coolwarm');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian Naive Bayes (GNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Bayes classifier\n",
    "* The Bayes classifier estimates conditional probabilities $\\mathbb P(Y=j|X=x_0)$ for all classes $j=1,\\ldots, k$. \n",
    "* An observation is assigned its most likely class, i.e., it is assigned to the class for which the conditional probability is greatest. \n",
    "* When the observation data is continuous, Gaussian naive Bayes is applied by calculating the conditional probability via a normal density, where the mean and variance are specific to each cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB()\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model.predict_proba(X).round(4)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pred==y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_score(y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "Xc = X[y==pred]\n",
    "Xf = X[y!=pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(x=Xc[:,0], y=Xc[:,1], c=y[y==pred], marker='o', cmap='coolwarm')\n",
    "plt.scatter(x=Xf[:,0], y=Xf[:,1], c=y[y!=pred], marker='x', cmap='coolwarm');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic regression (LR)\n",
    "* Logistic regression is a regression method where the dependent variable is a categorical variable (as opposed to a continuous variable). \n",
    "* It models the probability of the categorical variable given the independent variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LogisticRegression(C=1, solver='lbfgs')\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_proba(X).round(4)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "Xc = X[y==pred]\n",
    "Xf = X[y!=pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(x=Xc[:,0], y=Xc[:,1], c=y[y==pred], marker='o', cmap='coolwarm')\n",
    "plt.scatter(x=Xf[:,0], y=Xf[:,1], c=y[y!=pred], marker='x', cmap='coolwarm');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision trees\n",
    "* Decision tress classifiers can be thought of as a stepwise partitioning of the data space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(max_depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_proba(X).round(4)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "Xc = X[y==pred]\n",
    "Xf = X[y!=pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(x=Xc[:,0], y=Xc[:,1], c=y[y==pred], marker='o', cmap='coolwarm')\n",
    "plt.scatter(x=Xf[:,0], y=Xf[:,1], c=y[y!=pred], marker='x', cmap='coolwarm');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Increasing the maximum depth parameter for the decision tree allows to obtain a perfect result. \n",
    "* But note that overfitting may occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{:>8s} | {:8s}'.format('depth', 'accuracy'))\n",
    "print(20 * '-')\n",
    "for depth in range(1,7):\n",
    "    model = DecisionTreeClassifier(max_depth=depth)\n",
    "    model.fit(X,y)\n",
    "    acc = accuracy_score(y, model.predict(X))\n",
    "    print('{:8d} | {:8.2f}'.format(depth,acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep neural networks (DNN's)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep neural networks\n",
    "* DNN's are very powerful and versatile algorithms for estimation and for classification.\n",
    "* They are particularly useful for learning non-linear relationships, but this also makes them computationally demanding.\n",
    "* *Deep* refers to so-called hidden layers in the network, which makes them powerful, but can also make it hard to understand how a DNN operates (\"black box\").\n",
    "* `TensorFlow` is a popular open-source platform for DNN's. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier(solver = 'lbfgs', alpha=1e-5, hidden_layer_sizes=2 * [75],  random_state=10)\n",
    "%time model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score (y, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support vector machines (SVM's)\n",
    "* Aside from introducing SVM's we investigate how to split a data set into separate training and testing data sets.\n",
    "* A SVM is a classifier that linearly splits a hyperplane. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.33, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(C=1, kernel='linear')\n",
    "model.fit(train_x, train_y) # fit the model using the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(train_x) # predict the training data label values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(train_y, pred_train) # \"in-sample\" prediction rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pred_test = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y == pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(test_y, pred_test) # \"out-of-sample\" prediction rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "test_c = test_x[test_y == pred_test]\n",
    "test_f = test_x[test_y != pred_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(x=test_c[:,0], y=test_c[:,1], c=test_y[test_y == pred_test], marker='o', cmap='coolwarm')\n",
    "plt.scatter(x=test_f[:,0], y=test_f[:,1], c=test_y[test_y != pred_test], marker='x', cmap='coolwarm');"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_code_all_hidden": true,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "latex_metadata": {
   "author": "Prof.\\ Dr.\\ Natalie Packham\\\\ Berlin School of Economics and Law\\\\ Computational Finance and FinTech\\\\ Summer Term 2020",
   "bib": "notebook.bib",
   "title": "File I/O"
  },
  "livereveal": {
   "slideNumber": "c"
  },
  "toc": {
   "base_numbering": "10",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
